{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: date-parser-sari in /Users/fkraeutli/anaconda3/lib/python3.7/site-packages (0.8.11)\n",
      "Requirement already up-to-date: edtf in /Users/fkraeutli/anaconda3/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /Users/fkraeutli/anaconda3/lib/python3.7/site-packages (from edtf) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing in /Users/fkraeutli/anaconda3/lib/python3.7/site-packages (from edtf) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/fkraeutli/anaconda3/lib/python3.7/site-packages (from edtf) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install date-parser-sari edtf --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edtf import parse_edtf\n",
    "from sariDateParser.dateParser import parse\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"../data/source/sari_abzug-utf-8_23_04-tsv.json\"\n",
    "externalFieldsDirectory = \"../data/source/\"\n",
    "manifestDirectory = \"../data/manifests/\"\n",
    "outputDirectory = \"../data/xml/zbz/\"\n",
    "outputPrefix = \"zbz-record-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldsContainingDates = ['100$d', '260$c', '260$g', '264$c', '533$d', '600$d', '611$d', '700$d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "externalFields = ['100', '110', '264', '600', '610', '611', '650', '651', '655', '700', '710', '751']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 20\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertEDTFdate(date):\n",
    "    try:\n",
    "        d = parse_edtf(downgradeEDTF(date))\n",
    "    except:\n",
    "        raise ValueError('Invalid date', date)\n",
    "    \n",
    "    if 'Interval' in str(type(d)):\n",
    "        if type(d.lower) is list:\n",
    "            lower = d.lower[0].lower_strict()\n",
    "        else:\n",
    "            lower = d.lower.lower_strict()\n",
    "        if type(d.upper) is list:\n",
    "            upper = d.upper[0].upper_strict()\n",
    "        else:\n",
    "            upper = d.upper.upper_strict()\n",
    "    else:\n",
    "        if type(d) is list:\n",
    "            lower = d[0].lower_strict()\n",
    "            upper = d[0].upper_strict()\n",
    "        else:\n",
    "            lower = d.lower_strict()\n",
    "            upper = d.upper_strict()\n",
    "    return {\n",
    "        'lower': time.strftime(\"%Y-%m-%d\", lower),\n",
    "        'upper': time.strftime(\"%Y-%m-%d\", upper)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downgradeEDTF(date):\n",
    "    \"\"\"\n",
    "    Convert a edtf date string to the previous version supported by the python edtf package\n",
    "    \"\"\"\n",
    "    edtfDate = date.replace('X','u')\n",
    "    if edtfDate[-1:] == '/':\n",
    "        edtfDate += 'uuuu-uu'\n",
    "    if edtfDate[0] == '/':\n",
    "        edtfDate = 'uuuu-uu' + edtfDate\n",
    "    return edtfDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagesFromCachedManifest(manifest):\n",
    "    manifestFilePath = manifestDirectory + urllib.parse.quote(manifest, safe='') + '.json'\n",
    "    if os.path.isfile(manifestFilePath):\n",
    "        with open(manifestFilePath, 'r') as f:\n",
    "            content = json.load(f)\n",
    "            if 'sequences' in content and len(content['sequences']) > 0:\n",
    "                canvases = [d for d in content['sequences'][0]['canvases']]\n",
    "                images = [{\n",
    "                    'image': c['images'][0]['resource']['service']['@id'],\n",
    "                    'width': c['width'],\n",
    "                    'height': c['height']\n",
    "                } for c in canvases]\n",
    "                return images\n",
    "            else:\n",
    "                print(\"No sequences found in manifest %s\" % manifest)\n",
    "    else:\n",
    "        print(\"Manifest %s has not been cached\" % manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageListToXml(images):\n",
    "    imagesNode = etree.Element(\"images\")\n",
    "    for image in images:\n",
    "        imageNode = etree.SubElement(imagesNode, \"image\")\n",
    "        etree.SubElement(imageNode, \"height\").text = str(image['height'])\n",
    "        etree.SubElement(imageNode, \"width\").text = str(image['width'])\n",
    "        etree.SubElement(imageNode, \"url\", type=\"iiif\").text = image['image']\n",
    "    return imagesNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRowToXml(row, keys, externalFields):\n",
    "    record = etree.Element(\"record\")\n",
    "    etree.SubElement(record, \"uuid\").text = row['id']\n",
    "    etree.SubElement(record, \"record-identifier\").text = \"zbz-\" + row['001']\n",
    "    datafield = False\n",
    "    for key in keys:\n",
    "        # Check if key is a field that gets loaded externally (check only part before $ if present)\n",
    "        if key.split('$')[0] in externalFields.keys():\n",
    "            # Ignore the subfields as they will be loaded from the external fields\n",
    "            if not '$' in key:\n",
    "                # Select the field values based on the ids\n",
    "                fieldsToInclude = [d for d in externalFields[key] if d['id'] == row['id']]\n",
    "                for f in fieldsToInclude:\n",
    "                    # Create a datafield for each set of values\n",
    "                    datafield = etree.SubElement(record, \"datafield\", tag=key)\n",
    "                    for k in [d for d in f.keys()]:\n",
    "                        if key in k:\n",
    "                            code = k.split('_')[1].replace(' ','_')\n",
    "                        else:\n",
    "                            code = k.replace(' ','_')\n",
    "                        if f[k]:\n",
    "                            subfield = etree.SubElement(datafield, \"subfield\", code=code)\n",
    "                            subfield.text = str(f[k])\n",
    "                        # Check if field contains a date\n",
    "                        if k.replace(\"_\",\"$\") in fieldsContainingDates and f[k]:\n",
    "                            try:\n",
    "                                parsedDate = parse(f[k])\n",
    "                            except:\n",
    "                                print(\"Could not parse\", f[k],k)\n",
    "                            if parsedDate:\n",
    "                                subfield.set(\"parsedDate\", parsedDate)\n",
    "                                daterange = convertEDTFdate(parsedDate)\n",
    "                                subfield.set(\"upperDate\", daterange['upper'])\n",
    "                                subfield.set(\"lowerDate\", daterange['lower'])\n",
    "        else:\n",
    "            if key in row and row[key] is not None:\n",
    "                if '$' in key:\n",
    "                    code = key[4:]\n",
    "                    subfield = etree.SubElement(datafield, \"subfield\", code=code)\n",
    "                    subfield.text = str(row[key])\n",
    "                    # Check if field contains a date\n",
    "                    if key in fieldsContainingDates:\n",
    "                        parsedDate = parse(row[key])\n",
    "                        if parsedDate:\n",
    "                            subfield.set(\"parsedDate\", parsedDate)\n",
    "                            daterange = convertEDTFdate(parsedDate)\n",
    "                            subfield.set(\"upperDate\", daterange['upper'])\n",
    "                            subfield.set(\"lowerDate\", daterange['lower'])\n",
    "                    # Remove non-separated field content\n",
    "                    datafield.text = None\n",
    "                else:\n",
    "                    datafield = etree.SubElement(record, \"datafield\", tag=key)\n",
    "                    datafield.text = str(row[key])\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inputFile, 'r') as f:\n",
    "    rawData = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "externalFieldContent = {}\n",
    "for externalField in externalFields:\n",
    "    filePath = externalFieldsDirectory + externalField + '.json'\n",
    "    with open(filePath, 'r') as f:\n",
    "        try:\n",
    "            externalFieldContent[externalField] = json.load(f)['rows']\n",
    "        except:\n",
    "            exit(\"Could not read data from \" + filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(rawData['rows'][0].keys())\n",
    "keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def postProcess(record):\n",
    "    # Duplicate field 700 if there are several roles\n",
    "    datafields700 = record.findall(\"./datafield[@tag='700']\")\n",
    "    if len(datafields700):\n",
    "        for datafield in datafields700:\n",
    "            subfield4 = datafield.find(\"./subfield[@code='4']\")\n",
    "            subfieldE = datafield.find(\"./subfield[@code='e']\")\n",
    "            # If subfield 4 contains a comma, there are several roles defined\n",
    "            if subfield4 is not None and ',' in subfield4.text:\n",
    "                roleCodes = subfield4.text.split(', ')\n",
    "                roleNames = subfieldE.text.split(', ')\n",
    "                # Remove the field\n",
    "                datafieldTemplate = copy.copy(datafield)\n",
    "                datafield.getparent().remove(datafield)\n",
    "                # Create individual fields per role\n",
    "                for i, roleCode in enumerate(roleCodes):\n",
    "                    newDatafield = copy.copy(datafieldTemplate)\n",
    "                    newDatafield.find(\".subfield[@code='id_person']\").text = newDatafield.find(\".subfield[@code='id_person']\").text + \"-\" + str(i)\n",
    "                    newDatafield.find(\"./subfield[@code='4']\").text = roleCodes[i]\n",
    "                    newDatafield.find(\"./subfield[@code='e']\").text = roleNames[i]\n",
    "                    record.append(newDatafield)\n",
    "\n",
    "    return record\n",
    "\n",
    "# Output individual files\n",
    "for i, row in enumerate(tqdm(rawData['rows'][offset:limit+offset])):\n",
    "    \n",
    "    records = etree.Element(\"records\")\n",
    "    record = convertRowToXml(row, keys, externalFieldContent)\n",
    "    \n",
    "    if row['manifest']:\n",
    "        images = getImagesFromCachedManifest(row['manifest'])\n",
    "        if images:\n",
    "            record.append(imageListToXml(images))\n",
    "        else:\n",
    "            #print(\"Aborting due to missing manuscript\")\n",
    "            print(\"%d out of %d converted\" % (i, len(rawData['rows'])))\n",
    "            #exit()\n",
    "    \n",
    "    record = postProcess(record)\n",
    "    \n",
    "    records.append(record)\n",
    "    \n",
    "    outputFile = outputDirectory + outputPrefix + row['001'] + \".xml\"\n",
    "    with open(outputFile, 'wb') as f:\n",
    "        f.write(etree.tostring(records, xml_declaration=True, encoding='UTF-8', pretty_print=True))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
