{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "from lxml import etree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFiles = ['../data/source/nb-records.xml', '../data/source/nb-parentrecords.xml']\n",
    "outputDir = '../data/xml/nb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List externally loaded csv files\n",
    "# (these files contain data that has been added in open Refine)\n",
    "curatedDataFiles = [\n",
    "    \"../data/source/nb-curation-personen.csv\",\n",
    "    \"../data/source/nb-curation-koerperschaften.csv\",\n",
    "    \"../data/source/nb-curation-geografika.csv\"\n",
    "]\n",
    "curatedNamesFile = \"../data/source/nb-curation-names.csv\"\n",
    "curatedTypesFile = \"../data/source/nb-curation-extracted-types.csv\"\n",
    "imageSizesFile = \"../data/source/nb-image-sizes.csv\"\n",
    "externalDescriptorsFile = \"../data/source/nb-external-descriptors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column in CSV file used to match against IdName\n",
    "curatedKey = \"Raw\"\n",
    "\n",
    "# Columns to add\n",
    "curatedFieldsToAdd = [\"GND-Nummer\", \"GND-Kennung\", \"WD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input files\n",
    "root = etree.XML(\"<Collection/>\")\n",
    "for inputFile in inputFiles:\n",
    "    collection = etree.parse(inputFile)\n",
    "    for record in collection.findall(\"//Record\"):\n",
    "        root.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all Record elements. We will further work with this representation of the data\n",
    "records = root.findall(\"Record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records that either don't have an image or don't show up as a parent of another record\n",
    "parentIDs = []\n",
    "orphans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in records:\n",
    "    parentIDs.append(record.get('ParentId'))\n",
    "\n",
    "parentIDs = list(set(parentIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in records:\n",
    "    recordID = record.get('Id')\n",
    "    image = record.find('.//DataElement[@ElementId=\"11040\"]')\n",
    "    # If record contains no image and is not a parent of another record, mark as orphan\n",
    "    if image is None and recordID not in parentIDs:\n",
    "        orphans.append(recordID)\n",
    "\n",
    "records = [d for d in records if d.get('Id') not in orphans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all curated data into a list\n",
    "curatedData = []\n",
    "for curatedDataFile in curatedDataFiles:\n",
    "    with open(curatedDataFile, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            curatedData.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read curated types into separate list\n",
    "curatedTypes = []\n",
    "with open(curatedTypesFile, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        curatedTypes.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read curated names into a list\n",
    "curatedNames = []\n",
    "with open(curatedNamesFile, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        curatedNames.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each descriptor add curated data for available Thesaurus\n",
    "descriptors = root.xpath(\"//Descriptor\")\n",
    "\n",
    "for descriptor in descriptors:\n",
    "    thesaurus = descriptor.find(\"Thesaurus\").text\n",
    "    key = descriptor.find(\"IdName\").text\n",
    "    try:\n",
    "        dataToAdd = [d for d in curatedData if d['Thesaurus'] == thesaurus and d[curatedKey] == key][0]\n",
    "    except:\n",
    "        continue\n",
    "    for field in curatedFieldsToAdd:\n",
    "        if field in dataToAdd:\n",
    "            el = etree.SubElement(descriptor, field)\n",
    "            el.text = dataToAdd[field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14162/14162 [01:14<00:00, 191.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add curated type data\n",
    "for record in tqdm(records):\n",
    "    xpath = \"DetailData/DataElement[@ElementId=$ElementId]/ElementValue/TextValue[text()=$Term]\"\n",
    "    for row in curatedTypes:\n",
    "        fields = record.xpath(xpath, ElementId=row['ElementId'], Term=row['Term'])\n",
    "        if len(fields):\n",
    "            for field in fields:\n",
    "                parent = field.getparent()\n",
    "                for key in [d for d in row.keys() if d not in [\"ElementId\", \"ElementName\", \"Term\"]]:\n",
    "                    if row[key]:\n",
    "                        etree.SubElement(parent, key).text = row[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fields that contain the URL to the image on wikimedia (Element ID 11040) contain the same link twice \n",
    "# (often url encoded and not url encoded), separated by a newline. Here we remove the second value\n",
    "for record in records:\n",
    "    imageElement = record.find('.//DataElement[@ElementId=\"11040\"]')\n",
    "    if imageElement is not None and \"\\n\" in imageElement.find('./ElementValue/TextValue').text:\n",
    "        values = imageElement.find('./ElementValue/TextValue').text.split(\"\\n\")\n",
    "        imageElement.find('./ElementValue/TextValue').text = values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image sizes are stored in a separate CSV file, which we previously generated based on the IIIF Manifests.\n",
    "# We add the image sizes to the XML here\n",
    "imageSizes = []\n",
    "imageSizesHash = {}\n",
    "with open(imageSizesFile, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        imageSizes.append(row)\n",
    "        imageSizesHash[row['id']] = i\n",
    "        \n",
    "\n",
    "for record in records:\n",
    "    recordId = record.get('Id')\n",
    "    imageElement = record.find('.//DataElement[@ElementId=\"11040\"]/ElementValue')\n",
    "    if recordId in imageSizesHash:\n",
    "        sizes = imageSizes[imageSizesHash[recordId]]\n",
    "        etree.SubElement(imageElement, 'Width').text =  sizes['width']\n",
    "        etree.SubElement(imageElement, 'Height').text =  sizes['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process DataElements that have several values in one ElementValue by splitting the TextValue and adding extra ElementValues\n",
    "#\n",
    "# For example in ID 476941 the Element 10927 contains a TextValue that refers to two artists:\n",
    "#\n",
    "#            <DataElement ElementName=\"KünstlerIn\" ElementId=\"10927\" ElementType=\"Memo (max. 4000 Z.)\" ElementTypeId=\"7\">\n",
    "#              <ElementValue Sequence=\"1\">\n",
    "#                <TextValue>Aberli, Johann Ludwig [MalerIn/ZeichnerIn];\n",
    "#Zingg, Adrian [StecherIn]</TextValue>\n",
    "#               </ElementValue>\n",
    "#            </DataElement>\n",
    "#            \n",
    "# This should become:\n",
    "#\n",
    "#            <DataElement ElementName=\"KünstlerIn\" ElementId=\"10927\" ElementType=\"Memo (max. 4000 Z.)\" ElementTypeId=\"7\">\n",
    "#              <ElementValue Sequence=\"1-0\">\n",
    "#                <TextValue>Aberli, Johann Ludwig [MalerIn/ZeichnerIn]</TextValue>\n",
    "#              </ElementValue>\n",
    "#              <ElementValue Sequence=\"1-1\">\n",
    "#                <TextValue>Zingg, Adrian [StecherIn]</TextValue>\n",
    "#              </ElementValue>\n",
    "#            </DataElement>\n",
    "\n",
    "elementIdsWithMultipleNames = ['10817', '10927']\n",
    "dataElementXPath = '|'.join([\"DetailData/DataElement[@ElementId='%s']\" % d for d in elementIdsWithMultipleNames])\n",
    "\n",
    "for record in records:\n",
    "    dataElementsContainingNames = record.xpath(dataElementXPath)\n",
    "    if len(dataElementsContainingNames):\n",
    "        for dataElement in dataElementsContainingNames:\n",
    "            elementValues = dataElement.findall('./ElementValue')\n",
    "            for elementValue in elementValues:\n",
    "                text = elementValue.find(\"./TextValue\").text\n",
    "                if \";\" in text:\n",
    "                    # Extract data\n",
    "                    values = text.split(\";\\n\")\n",
    "                    sequence = elementValue.get(\"Sequence\")\n",
    "                    # Remove ElementValue\n",
    "                    dataElement.remove(elementValue)\n",
    "                    # Create new ElementValue elements for each value\n",
    "                    for i, value in enumerate(values):\n",
    "                        newElementValue = etree.SubElement(dataElement, \"ElementValue\")\n",
    "                        newElementValue.set(\"Sequence\", \"%s-%d\" % (sequence, i))\n",
    "                        newTextValue = etree.SubElement(newElementValue, \"TextValue\")\n",
    "                        newTextValue.text = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names in the descriptors do not exactly match the ones used in the DataFields, for example...\n",
    "#\n",
    "#   <DataElement ElementName=\"KünstlerIn\" ElementId=\"10927\" ElementType=\"Memo (max. 4000 Z.)\" ElementTypeId=\"7\">\n",
    "#     <ElementValue Sequence=\"1\">\n",
    "#       <TextValue>Keller, Hans Heinrich</TextValue>\n",
    "#     </ElementValue>\n",
    "#   </DataElement>\n",
    "#\n",
    "#  ...has the corresponding Person Element...\n",
    "#\n",
    "#   <DataElement ElementName=\"Personen\" ElementId=\"11053\" ElementType=\"Memo (max. 4000 Z.)\" ElementTypeId=\"7\">\n",
    "#     <ElementValue Sequence=\"1\">\n",
    "#       <TextValue>BildendeR KünstlerIn  / Personen / K / Keller, Hans Heinrich / 1778 - 1862</TextValue>\n",
    "#     </ElementValue>\n",
    "#   </DataElement>\n",
    "#\n",
    "#  ...and Descriptor\n",
    "#\n",
    "#   <Descriptor>\n",
    "#     <Name>BildendeR KünstlerIn</Name>\n",
    "#     <Thesaurus>Personen</Thesaurus>\n",
    "#     <IdName>BildendeR KünstlerIn  (Personen\\K\\Keller, Hans Heinrich (1778 - 1862))</IdName>\n",
    "#     <SeeAlso>Keller, Hans Heinrich (1778 - 1862)</SeeAlso>\n",
    "#     <GND-Nummer>1018634584</GND-Nummer>\n",
    "#   </Descriptor>\n",
    "#\n",
    "#  Neither the Person Element nor the Descriptor specify the role exactly. They define the person as BilndendeR Künstlerin,\n",
    "#  but the DataElement can be more specific with regardsd to KünstlerIn or FotografIn (based on current data).\n",
    "#  \n",
    "#  Therefore, the Descriptor is added to the relevant DataElement by checking for the occurrence of the TextValue\n",
    "#  (e.g. Keller, Hans Heinrich) in the Descriptor's IdName (e.g. BildendeR KünstlerIn  (Personen\\K\\Keller, Hans Heinrich (1778 - 1862)))\n",
    "\n",
    "# Element IDs in which such names appear\n",
    "\n",
    "# Helper functions for matching the names and roles\n",
    "def cleanName(name):\n",
    "    return re.sub(r'[^A-Za-z]+', '', name)\n",
    "\n",
    "def matchNameWithCuratedNames(name, curatedNames):\n",
    "\n",
    "    def NFD(s):\n",
    "        return unicodedata.normalize('NFD', s)\n",
    "\n",
    "    for curatedName in curatedNames:\n",
    "        if NFD(name) in NFD(curatedName['Raw']):\n",
    "            return curatedName['normalised name']\n",
    "            \n",
    "    print(\"Not found \", name)\n",
    "    return False\n",
    "\n",
    "def matchRoleWithCuratedNames(name, curatedNames):\n",
    "    # We use the name list to match roles as well. Eventually one could use a smaller list of only the roles as well\n",
    "    for curatedName in curatedNames:\n",
    "        if curatedName['normalised role'] and curatedName['Role'] in name:\n",
    "            roles = curatedName['normalised role'].split(\"/\") \n",
    "            gndRoles = curatedName['gnd role'].split(\";\")\n",
    "            returnRoles = []\n",
    "            for i in range(min(len(roles), len(gndRoles))):\n",
    "                returnRoles.append({\"label\": roles[i], \"gnd\": gndRoles[i]})\n",
    "            return returnRoles\n",
    "    return False\n",
    "\n",
    "class NBExternalDescriptors:\n",
    "    import csv\n",
    "    \n",
    "    allPersonDescriptors = []\n",
    "    personDescriptorIdNameHash = {}\n",
    "    externalDescriptors = []\n",
    "    externalDescriptorFilename = \"\"\n",
    "    \n",
    "    def __init__(self, records, filename):\n",
    "        from os import path\n",
    "        # Read all Person descriptors\n",
    "        for record in records:\n",
    "            recordDescriptors = record.xpath(\"Descriptors/Descriptor[Thesaurus/text()='Personen']\")\n",
    "            for descriptor in recordDescriptors:\n",
    "                idName = descriptor.find(\"IdName\").text\n",
    "                if idName not in self.personDescriptorIdNameHash:\n",
    "                    self.personDescriptorIdNameHash[idName] = len(self.allPersonDescriptors)\n",
    "                    self.allPersonDescriptors.append(descriptor)\n",
    "                    \n",
    "        # Read external descriptors\n",
    "        self.externalDescriptorFilename = filename\n",
    "        if path.isfile(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    self.externalDescriptors.append(row)\n",
    "    \n",
    "    def cleanName(name):\n",
    "        return re.sub(r'[^A-Za-z]+', '', name)\n",
    "\n",
    "    def getDescriptorForRecordAndName(self, recordId, name):\n",
    "        for externalDescriptor in self.externalDescriptors:\n",
    "            if externalDescriptor['recordId'] == recordId and externalDescriptor['matchedName'] == name:\n",
    "                return self.allPersonDescriptors[self.personDescriptorIdNameHash[externalDescriptor['idName']]]\n",
    "        return False\n",
    "    \n",
    "    def getPersonDescriptorByName(self, recordId, name):\n",
    "        for descriptor in self.allPersonDescriptors:\n",
    "            idName = descriptor.find(\"IdName\").text\n",
    "            if cleanName(name) in cleanName(idName):\n",
    "                if not self.getDescriptorForRecordAndName(recordId, name):\n",
    "                    self.addExternalDescriptor(recordId, matchedName, idName)\n",
    "                return descriptor\n",
    "        return False\n",
    "    \n",
    "    def addExternalDescriptor(self, recordId, matchedName, idName):\n",
    "        self.externalDescriptors.append({\n",
    "            \"recordId\": recordId,\n",
    "            \"matchedName\": matchedName,\n",
    "            \"idName\": idName\n",
    "        })\n",
    "        self.writeExternalDescriptors()\n",
    "    \n",
    "    def writeExternalDescriptors(self):\n",
    "        externalDescriptors = sorted(self.externalDescriptors, key=lambda k: k['recordId']) \n",
    "        with open(self.externalDescriptorFilename, 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=externalDescriptors[0].keys())\n",
    "            writer.writeheader()\n",
    "            for row in externalDescriptors:\n",
    "                writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fkraeutli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "/Users/fkraeutli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found  Knig, Franz Niklaus\n",
      "Unmatched name in Record 479142\n",
      "Not found  Muller, Thodore [LithografIn]\n",
      "Unmatched name in Record 891180\n"
     ]
    }
   ],
   "source": [
    "elementIdsWithCuratedNames = ['10817', '10927']\n",
    "dataElementXPath = '|'.join([\"DetailData/DataElement[@ElementId='%s']\" % d for d in elementIdsWithCuratedNames])\n",
    "\n",
    "externalDescriptors = NBExternalDescriptors(records, externalDescriptorsFile)\n",
    "\n",
    "# Find a match for each person and add curate data on role\n",
    "for record in records:\n",
    "    \n",
    "    # Extract Elements containing names\n",
    "    recordElements = record.xpath(dataElementXPath)\n",
    "    recordDescriptors = record.xpath(\"Descriptors/Descriptor[Thesaurus/text()='Personen']\")\n",
    "    \n",
    "    if len(recordElements):\n",
    "        for recordElement in recordElements:\n",
    "            # Extract ElementValues (there can be several)\n",
    "            values = recordElement.xpath(\"ElementValue\")\n",
    "            for value in values:\n",
    "                name = value.find(\"TextValue\").text\n",
    "\n",
    "                matchedName = matchNameWithCuratedNames(name, curatedNames)\n",
    "                if matchedName:\n",
    "                    # If a match is found, copy the descriptor directly into the Element\n",
    "                    for descriptor in recordDescriptors:\n",
    "                        idName = descriptor.find(\"IdName\").text\n",
    "                        if cleanName(matchedName) in cleanName(idName):\n",
    "                            value.append(copy.deepcopy(descriptor))\n",
    "                            break\n",
    "                    else:\n",
    "                        # Sometimes no descriptor is present together with the record\n",
    "                        # but there are matching descriptors elswehere in the dataset.\n",
    "                        # Here we look for a suitable descriptors among all of them\n",
    "                        descriptor = externalDescriptors.getDescriptorForRecordAndName(record.get('Id'), matchedName)\n",
    "                        if not descriptor:\n",
    "                            descriptor = externalDescriptors.getPersonDescriptorByName(record.get('Id'), matchedName)\n",
    "                        if descriptor:\n",
    "                            value.append(copy.deepcopy(descriptor))\n",
    "                    \n",
    "                    # Add a normalised name so we can create a single entity for\n",
    "                    # persons that lack a GND identifier\n",
    "                    normalisedName = etree.SubElement(value, \"NormalisedName\")\n",
    "                    normalisedName.text = matchedName\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Unmatched name in Record\", record.get('Id'))\n",
    "\n",
    "                matchedRoles = matchRoleWithCuratedNames(name, curatedNames)\n",
    "                if matchedRoles:\n",
    "                    for role in matchedRoles:\n",
    "                        roleElement = etree.SubElement(value, \"Role\")\n",
    "                        roleElement.set(\"gnd\", role['gnd'])\n",
    "                        roleElement.text = role['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records contain date description as date ranges\n",
    "#\n",
    "# Date ranges are specified using different Date Operators, for example\n",
    "# whether a date range specifes an exact date, or a period, before or after a date.\n",
    "# However, if the date is not a range, but a single date, only the FromDate\n",
    "# tag is filled, whether or not the date refers to a beginnig or end of an (unknown)\n",
    "# range. For easier mapping, we move some of the dates to the ToDate tag, for example\n",
    "# when a date Range is specified as \"before\", we want the date to be in ToDate, but not\n",
    "# in FromDate\n",
    "#\n",
    "# We also add the full date information for easier representation as xsd:date later\n",
    "\n",
    "def getDateForDateElement(date):\n",
    "    \"\"\"\n",
    "        Add day and month information for years\n",
    "        Jan 1 or Dec 31 depending on whether it is a beginning or end date\n",
    "    \"\"\"\n",
    "    if not date.text:\n",
    "        return False\n",
    "        \n",
    "    patternCeYear = r'\\+\\d{4}'\n",
    "    if re.match(patternCeYear, date.text):\n",
    "        year = date.text[1:].zfill(4)\n",
    "        if date.tag == 'FromDate':\n",
    "            return \"%s-01-01\" % year\n",
    "        else:\n",
    "            return \"%s-12-31\" % year\n",
    "    return False\n",
    "\n",
    "# Switch from/to dates for \"before\" and \"To\" date ranges\n",
    "dateRanges = root.xpath(\"//DateRange\")\n",
    "for dateRange in dateRanges:\n",
    "    operator = dateRange.get(\"DateOperator\")\n",
    "    if operator == \"before\" or operator == \"To\":\n",
    "        fromDate = dateRange.find(\"FromDate\")\n",
    "        toDate = dateRange.find(\"ToDate\")\n",
    "        toDate.text = fromDate.text\n",
    "        fromDate.text = \"\"\n",
    "\n",
    "# For each date element add full date information\n",
    "dates = root.xpath(\"//FromDate|//ToDate\")\n",
    "for date in dates:\n",
    "    fullDate = getDateForDateElement(date)\n",
    "    if fullDate:\n",
    "        date.set(\"fullDate\", fullDate)\n",
    "\n",
    "collection = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14162/14162 [00:00<00:00, 70723.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/xml/nb/nb-record-938014.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 100000\n",
    "offset = 0\n",
    "\n",
    "idsToExtract = ['938014']\n",
    "# Output each record individually\n",
    "for record in tqdm(records[offset:limit+offset]):\n",
    "    collection.clear()\n",
    "    id = record.get(\"Id\")\n",
    "    if id in idsToExtract:\n",
    "        parentId = record.get(\"ParentId\")\n",
    "        record.set(\"RecordIdentifier\", \"nb-\" + id)\n",
    "        record.set(\"ParentRecordIdentifier\", \"nb-\" + parentId)\n",
    "        collection.append(record)\n",
    "        outputFile = \"%s/nb-record-%s.xml\" % (outputDir, id)\n",
    "        print(outputFile)\n",
    "        with open(outputFile, 'wb') as f:\n",
    "            f.write(etree.tostring(collection, xml_declaration=True, pretty_print=True, encoding=\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
